{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMPUTE LU - AI for Medicine and Life Science\n",
    "### Lab: Image Classification for the HAM10000 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab is about using transfer learning to learn to predict on the HAM10000 dataset. We have picked two models VGG16 and ResNet50 - you can of course choose others.\n",
    "\n",
    "You can run this notebook locally if you wish, but a GPU is preferred.\n",
    "\n",
    "You will need to implement some code, these are marked with comments:\n",
    "\n",
    "```py\n",
    "# your code here\n",
    "```\n",
    "\n",
    "### Goal\n",
    " \n",
    "  * Train one VGG16 and one ResNet50 model that achieves above 50% accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HAM10000\n",
    "The HAM10000 dataset is a large collection of multi-source dermatoscopic images of common pigmented skin lesions\n",
    "\n",
    "Tschandl, Philipp, 2018, \"The HAM10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions\", https://doi.org/10.7910/DVN/DBW86T, Harvard Dataverse, V3, UNF:6:/APKSsDGVDhwPBWzsStU5A== [fileUNF]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More information: https://www.nature.com/articles/sdata2018161"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset contains images of skin lesions with an associated class that represents the type of skin lesion:\n",
    "\n",
    " * **akiec** - Actinic keratoses and intraepithelial carcinoma / Bowen's disease, \n",
    " * **bcc** - basal cell carcinoma,\n",
    " * **blk** - benign keratosis-like lesions (solar lentigines / seborrheic keratoses and lichen-planus like keratoses), \n",
    " * **df** - dermatofibroma, \n",
    " * **mel** - melanoma, \n",
    " * **nv** - melanocytic nevi\n",
    " * **vasc** - vascular lesions (angiomas, angiokeratomas, pyogenic granulomas and hemorrhage)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Imports and packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CoLab requires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U scikit-learn imbalanced-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All other packages are up to date in the CoLab environemtn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numeric operations\n",
    "import numpy as np\n",
    "\n",
    "# reading csv\n",
    "import pandas as pd\n",
    "\n",
    "# machine learning\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras import layers, optimizers, losses, applications, callbacks, Model, Sequential\n",
    "\n",
    "# oversampling\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# plotting utillities\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "# progress\n",
    "from tqdm import tqdm\n",
    "\n",
    "# notebook utility\n",
    "from IPython.display import display\n",
    "\n",
    "# evaluation\n",
    "from sklearn.metrics import classification_report, PrecisionRecallDisplay, confusion_matrix\n",
    "\n",
    "# simple counter\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Download and unpack data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://fileadmin.cs.lth.se/dataset/HAM10000-compute-lu-lab.tar.gz\n",
    "!tar -xvzf HAM10000-compute-lu-lab.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read metadata which is a csv with comma as the seperator, the first line is the header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv(\"HAM10000_dataset/metadata.csv\", sep=',', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_categorical = pd.Categorical(metadata[\"dx\"], ordered=True)\n",
    "id2label = dict( enumerate(label_categorical.categories ) )\n",
    "labels = label_categorical.categories.to_list()\n",
    "\n",
    "metadata[\"label\"] = label_categorical.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle the lesions by lesion_id to make sure that different images of the lesion does not appear in train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_lesions = pd.Series(metadata[\"lesion_id\"].unique(), name=\"lesion_id\").sample(frac=1, random_state=1667)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_lesions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Create splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will pick a 60/20/20 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_start, val_start, test_start = 0, int(len(shuffled_lesions)*0.6), int(len(shuffled_lesions)*0.8)\n",
    "\n",
    "train_metadata_split = metadata[metadata[\"lesion_id\"].isin(shuffled_lesions[0:val_start])]\n",
    "val_metadata_split = metadata[metadata[\"lesion_id\"].isin(shuffled_lesions[val_start:test_start])]\n",
    "test_metadata_split = metadata[metadata[\"lesion_id\"].isin(shuffled_lesions[test_start:])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 What is the distribution of classes?\n",
    "Number of unique entries by label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_distribution = train_metadata_split.groupby(\"label\").nunique()[\"image_id\"]\n",
    "label_distribution / label_distribution.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above we can se that classes 0, 3 and 6 fall below the 5% mark, which is typically a rule of thumb of when classes coud be ignored by the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadata_split.groupby(\"label\").nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_metadata_split.groupby(\"label\").nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metadata_split.groupby(\"label\").nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The data is very imbalanced**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Trying to mitigate the imbalance\n",
    "We randomly oversample the minority classes so that we have an even distribution of images.\n",
    "RandomOverSampler picks the same image multiple times but in a random order.\n",
    "\n",
    "The idea to use image ids is because these do not take much memory, we can stream in the same image multiple times instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ros = RandomOverSampler(random_state=1667)\n",
    "train_image_ids, train_labels = ros.fit_resample(train_metadata_split[\"image_id\"].to_numpy().reshape(-1,1), train_metadata_split[\"label\"].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, count in sorted(Counter(train_labels).items()):\n",
    "    print(f\"{labels[label]} = {count}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perfectly balanced** - maybe not ideal, you can change this outcome by giving a number of images per class to RandomOverSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Create tensorflow datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = tf.data.Dataset.from_tensor_slices((train_image_ids.flatten(), train_labels))\n",
    "ds_val = tf.data.Dataset.from_tensor_slices((val_metadata_split[\"image_id\"].to_numpy(), val_metadata_split[\"label\"].to_numpy()))\n",
    "ds_test = tf.data.Dataset.from_tensor_slices((test_metadata_split[\"image_id\"].to_numpy(), test_metadata_split[\"label\"].to_numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Create the loading function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pseudo code:\n",
    " * Read the image\n",
    " * Decode the raw data as jpeg\n",
    " * Convert to float32 using `tf.cast`\n",
    " * Resize to 224, 224\n",
    " * Return image and label as a tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will be compiled by tensorflow into the graph\n",
    "\n",
    "@tf.function()\n",
    "def prepare(imagename, label):\n",
    "    \n",
    "    # your code here\n",
    "\n",
    "    return (None,None)  # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Test the loading function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,16))\n",
    "for i, (X, y) in enumerate(ds_train.shuffle(10000, seed=1667).map(prepare).take(16).as_numpy_iterator()):\n",
    "    sub = fig.add_subplot(4,4, i+1)\n",
    "    sub.title.set_text(\"Label \" + id2label[y])\n",
    "    sub.imshow(X/255.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark the data loader\n",
    "This also verify that all required images loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "num_images = 0\n",
    "for X,y in tqdm(ds_train.shuffle(30000).map(prepare, num_parallel_calls=tf.data.AUTOTUNE).as_numpy_iterator()):\n",
    "    num_images += 1\n",
    "\n",
    "print(num_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Image classification with a pretrained VGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HAM10000 dataset is a realistc but hard to get good predictions for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Create dataset iterators\n",
    "\n",
    "We shuffle training by imageid to save memory and we do not cache it - we stream it in   \n",
    "Valdigation and test are cached for speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = ds_train.shuffle(30000).map(prepare, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "val  = ds_val.map(prepare, num_parallel_calls=tf.data.AUTOTUNE).cache().shuffle(10000)\n",
    "test = ds_test.map(prepare, num_parallel_calls=tf.data.AUTOTUNE).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Define the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a network with the following suggested aritecture / try another if you like:\n",
    " \n",
    "  * Input of 224x224x3\n",
    "  * Pretrained VGG16 - freeze the model, no finetuning\n",
    "     + Do not forget preprocessing the input using VGG16.preprocess_input\n",
    "  * Flatten its output\n",
    "  * Choose a number of hidden layers before the final prediction\n",
    "  * final classification layer - 7 labels, with softmax activation\n",
    "\n",
    "Regarding layer choices, you may get some inspiration from here:\n",
    "https://towardsdatascience.com/illustrated-10-cnn-architectures-95d78ace614d\n",
    "\n",
    "#### Examples\n",
    "The live coding session:\n",
    "https://github.com/COMPUTE-LU/AI4MedLife_imaging_2021/blob/main/slides/Ai4MedLife_imaging_day2_3/LiveCoding.ipynb\n",
    "\n",
    "#### Documentation\n",
    "*transfer models* - https://keras.io/api/applications/   \n",
    "*layers* - https://keras.io/api/layers/   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vgg16_model():\n",
    "    # your code here\n",
    "    \n",
    "    return None # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train it with RMSprop with an inital learning rate of 3e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg16 = create_vgg16_model()\n",
    "model_vgg16.compile(optimizer=optimizers.RMSprop(3e-5), loss=losses.SparseCategoricalCrossentropy(), metrics=[\"accuracy\"])\n",
    "model_vgg16.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe the number of trainable parameters, there should be a large number of non-trainable parameters because the vgg16 is supposed to be frozen/not trainable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Train the model for 10 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`train` is a `tf.data.Dataset`, use its methods to take 8000 images, batch them into 32 images at a time and as a final operation in the chain: prefetch 8 for performance.\n",
    "\n",
    "Use Model Checkpointing to save the best model time.  \n",
    "Run the full 10 epochs. Load the best model after training.\n",
    " \n",
    "*Hint:* ModelCheckpoint is constructed and added to the callbacks list of Model.fit\n",
    "\n",
    "#### Documentation\n",
    "`tf.data.Dataset` - https://www.tensorflow.org/api_docs/python/tf/data/Dataset   \n",
    "`Model.fit` - https://keras.io/api/models/model_training_apis/#fit-method   \n",
    "`ModelCheckpoint` - https://keras.io/api/callbacks/model_checkpoint/   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "history_vgg16_ep10 = None # your code here: history as returned by `Model.fit`\n",
    "model_vgg16_best = None # your code here: replace with your best model after training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Evaluate\n",
    "\n",
    " * Plot loss curve\n",
    " * Compute confusion matrix\n",
    " * Compute f1 score \n",
    " * Plot precision/recall curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, testset, labels):\n",
    "    \n",
    "    # predict outputs for testset\n",
    "    y_pred_prob = None # your code here\n",
    "\n",
    "    # find the best class for each prediction\n",
    "    # input = (samples,classes)\n",
    "    # output = (samples,), where each element is the best class\n",
    "    y_pred = None # your code here\n",
    "\n",
    "    y_true = np.array([y for X,y in testset.as_numpy_iterator()])\n",
    "\n",
    "    print(\"Classification Report\")\n",
    "    print(classification_report(y_true, y_pred, labels=list(range(7)), target_names=labels))\n",
    "\n",
    "    print(\"Confusion matrix\")\n",
    "    display(pd.DataFrame(confusion_matrix(y_true, y_pred), columns=labels, index=labels))\n",
    "\n",
    "    fig = plt.figure(figsize=(16,8))\n",
    "    for i in range(7):\n",
    "        ax = fig.add_subplot(2,4, i+1)\n",
    "        PrecisionRecallDisplay.from_predictions(y_true == i, y_pred_prob[:,i], name=labels[i], ax = ax)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_curves(loss, val_loss, title=\"Learning curves\"):\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.title(title)\n",
    "    n = len(loss)\n",
    "    plt.plot(range(1, n+1), loss, 'b',label=\"loss\")\n",
    "    plt.plot(range(1, n+1), val_loss, 'r', label=\"val_loss\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_curves(history_vgg16_ep10.history[\"loss\"], history_vgg16_ep10.history[\"val_loss\"], \"VGG16 - 10ep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model_vgg16_best, test, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Train the model longer\n",
    "\n",
    "Continue from the previous model, run for 10 more epochs.\n",
    "\n",
    "Set the `initial_epoch` argument of `Model.fit`\n",
    "\n",
    "#### Documentation\n",
    "`Model.fit` - https://keras.io/api/models/model_training_apis/#fit-method   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "history_vgg16_longer = None # your code here: history as returned by `Model.fit`\n",
    "model_vgg16_best = None # your code here: replace with your best model after training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Evaluate\n",
    "Same as in 3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_curves(history_vgg16_ep10.history[\"loss\"] + history_vgg16_longer.history[\"loss\"], history_vgg16_ep10.history[\"val_loss\"] + history_vgg16_longer.history[\"val_loss\"], \"VGG16 - 20ep\")\n",
    "evaluate_model(model_vgg16_best, test, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Compare loss curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_two_learning_curves(history1, history2, name1, name2, title=\"Comparing loss curves\", history2_from_epoch=None):\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.title(title)\n",
    "    n = len(history1.history[\"loss\"])\n",
    "    plt.plot(range(1, n+1), history1.history[\"loss\"], 'b',label=f\"loss ({name1})\")\n",
    "    plt.plot(range(1, n+1), history1.history[\"val_loss\"], 'r', label=f\"val_loss ({name1})\")\n",
    "\n",
    "    start = 1\n",
    "    if history2_from_epoch is not None:\n",
    "        start = history2_from_epoch\n",
    "\n",
    "    n = len(history2.history[\"loss\"])\n",
    "    plt.plot(range(start, n+start), history2.history[\"loss\"], 'g',label=f\"loss ({name2})\")\n",
    "    plt.plot(range(start, n+start), history2.history[\"val_loss\"], 'm', label=f\"val_loss ({name2})\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_two_learning_curves(history_vgg16_ep10, history_vgg16_ep20, \"10ep\", \"20ep\", \"Comparing VGG16 at 10 and 20 ep\", history2_from_epoch=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Image classification with a pretrained ResNet-50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Construct the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take inspiration from how it originally was trained:\n",
    "https://towardsdatascience.com/illustrated-10-cnn-architectures-95d78ace614d\n",
    "\n",
    "**Hint:** The last two layers (GlobalAveragePooling2D) and a dense layer   \n",
    "**Hint:** Modify and adjust your solution from 3.2.\n",
    "\n",
    "#### Documentation\n",
    "*transfer models* - https://keras.io/api/applications/   \n",
    "*layers* - https://keras.io/api/layers/   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_resnet50_model():\n",
    "    # your code here\n",
    "\n",
    "    return None # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Train and evalute the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train for 20 epochs with model checkpointing and early stopping with a patience of 5\n",
    "\n",
    "Take a look at:\n",
    "https://keras.io/api/callbacks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "history_resnet50 = None # your code here: history as returned by `Model.fit`\n",
    "model_resnet50_best = None # your code here: replace with your best model after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_curves(history_resnet50.history[\"loss\"],history_resnet50.history[\"val_loss\"], \"ResNet50\")\n",
    "evaluate_model(model_resnet50_best, test, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Implement augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Create a function that creates a augmentation layer with hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the following augmentation steps:\n",
    " * Random Translation\n",
    " * Random Rotation\n",
    " * Random Zoom\n",
    " * Random Flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_augmentation(translate=(0.1,0.1), rotation=0.25, zoom=(0.1,0.1)):\n",
    "    augmentation = Sequential(name=\"augmentation\")\n",
    "    augmentation.add(layers.RandomTranslation(translate[0], translate[1]))\n",
    "    augmentation.add(layers.RandomRotation(rotation))\n",
    "    augmentation.add(layers.RandomZoom(zoom[0], zoom[1]))\n",
    "    augmentation.add(layers.RandomFlip())\n",
    "    return augmentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Test it on some images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation = create_augmentation()\n",
    "\n",
    "fig = plt.figure(figsize=(16,16))\n",
    "for i, (X, y) in enumerate(ds_train.shuffle(10000, seed=1667).map(prepare).take(16).as_numpy_iterator()):\n",
    "    sub = fig.add_subplot(4,4, i+1)\n",
    "    sub.title.set_text(\"Label \" + id2label[y])\n",
    "    sub.imshow(augmentation(X)/255.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Train VGG16 or ResNet-50 with augmented data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Construct and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Expand and experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Things to try\n",
    "\n",
    " * Change optimizer / learning rate\n",
    " * Pick other transfer learning models\n",
    " * Add Learning Rate scheduler: ReduceLROnPlateau\n",
    " * Adjust amount / Remove dropout\n",
    " * Change batch size\n",
    " * Change imbalance sampling settings\n",
    " * Change number of final layers, their widths\n",
    " * Replace GlobalAverage2D with Flatten\n",
    " * Unfreeze transfer learned layer and also fine tune\n",
    " * Implement a custom callback that logs the loss for every batch\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "abaf542429bb04ff3527848ea2190c1fded34c6cbddadedd2ad9519f81222d32"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
